{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AXpjWmedCiPI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/"
      ],
      "metadata": {
        "id": "N3M0LQ52Cmaf"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/bin/bash\n",
        "!kaggle datasets download zalando-research/fashionmnist"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikA2aEDrC1bn",
        "outputId": "a98e5385-8b85-4991-cf5d-46b9ee84861f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "Dataset URL: https://www.kaggle.com/datasets/zalando-research/fashionmnist\n",
            "License(s): other\n",
            "fashionmnist.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "zip_ref = zipfile.ZipFile('/content/fashionmnist.zip')  # Corrected typo and path\n",
        "zip_ref.extractall('/content')  # Corrected path\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "L_P0gYE8C41-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dT5zNSZsC-oa",
        "outputId": "833cfa94-9118-4e61-f746-83c844c41813"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f8f02ae0c30>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3MkE4FKDVkR",
        "outputId": "ad59f6b9-ef3c-42d9-8f47-0482023574d7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('/content/fashion-mnist_train.csv')\n",
        "test = pd.read_csv('/content/fashion-mnist_test.csv')"
      ],
      "metadata": {
        "id": "j4KdUrYzDa7N"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = train.iloc[:,1:]\n",
        "y_train = train.iloc[:,0]\n",
        "x_test = test.iloc[:,1:]\n",
        "y_test = test.iloc[:,0]"
      ],
      "metadata": {
        "id": "eD-Q3e_3D1qf"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YPcHOUX6EcXE"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.transforms import transforms\n",
        "custom_transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485,0.456,0.406],std=[0.229,0.224,0.225])\n",
        "])"
      ],
      "metadata": {
        "id": "ZkmfCoO8Krl0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "LcoDQ7y9Q1mz"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "\n",
        "  def __init__(self,features,labels,transform):\n",
        "    self.features = features.values\n",
        "    self.labels = labels\n",
        "    self.transform = transform\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.features)\n",
        "\n",
        "  def __getitem__(self,index):\n",
        "    image = self.features[index].reshape(28,28)\n",
        "    image = Image.fromarray(image.astype(np.uint8), mode='L').convert('RGB')\n",
        "\n",
        "\n",
        "    image = self.transform(image)\n",
        "\n",
        "    return image,torch.tensor(self.labels[index],dtype=torch.long)"
      ],
      "metadata": {
        "id": "i_Hp5xt-E4UD"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = CustomDataset(x_train,y_train,transform=custom_transform)\n",
        "test_dataset = CustomDataset(x_test,y_test,transform=custom_transform)\n"
      ],
      "metadata": {
        "id": "Vu9TW1QfH9OZ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset,batch_size=32,shuffle=True)\n",
        "test_loader = DataLoader(test_dataset,batch_size=32,shuffle=False)"
      ],
      "metadata": {
        "id": "D6oDrWvaJUMk"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.models as models\n",
        "vgg16 = models.vgg16(pretrained = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-E0Vqv3eJ3sZ",
        "outputId": "3b3969a0-f54f-470a-e7fa-d6b7e3372bda"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for param in vgg16.features.parameters():\n",
        "  param.requires_grad=False"
      ],
      "metadata": {
        "id": "xmqaWREYVEfV"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vgg16.classifier = nn.Sequential(\n",
        "    nn.Linear(25088,1024),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(1024,512),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(512,10),\n",
        ")"
      ],
      "metadata": {
        "id": "45l-y_wlVTBD"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vgg16 = vgg16.to(device)"
      ],
      "metadata": {
        "id": "tU3tBRyoWC0G"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.0001\n",
        "epochs = 10"
      ],
      "metadata": {
        "id": "kQeg1Lf3WMUX"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(vgg16.classifier.parameters(),lr = learning_rate,weight_decay=1e-4)"
      ],
      "metadata": {
        "id": "jj5GbIU-Wbsp"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):  # Loop over the dataset multiple times\n",
        "  total_epoch_loss = 0\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  for x, y in train_loader:\n",
        "    x, y = x.to(device), y.to(device)\n",
        "\n",
        "    # forward prop\n",
        "    outputs = vgg16(x)\n",
        "    # calculate loss\n",
        "    loss = criterion(outputs, y)\n",
        "    # back prop\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    # update grads\n",
        "    optimizer.step()\n",
        "\n",
        "    total_epoch_loss += loss.item()\n",
        "\n",
        "    # Calculate accuracy\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    total += y.size(0)\n",
        "    correct += (predicted == y).sum().item()\n",
        "\n",
        "  avg_loss = total_epoch_loss / len(train_loader)\n",
        "  accuracy = 100 * correct / total\n",
        "  print(f\"Epoch: {epoch + 1}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSIzS3LedJo2",
        "outputId": "be471acb-e025-4711-b03f-0b21aed2a61d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Loss: 0.3468, Accuracy: 87.94%\n",
            "Epoch: 2, Loss: 0.2140, Accuracy: 92.39%\n",
            "Epoch: 3, Loss: 0.1703, Accuracy: 93.82%\n",
            "Epoch: 4, Loss: 0.1395, Accuracy: 94.98%\n",
            "Epoch: 5, Loss: 0.1160, Accuracy: 95.86%\n",
            "Epoch: 6, Loss: 0.0967, Accuracy: 96.51%\n",
            "Epoch: 7, Loss: 0.0835, Accuracy: 96.97%\n",
            "Epoch: 8, Loss: 0.0747, Accuracy: 97.32%\n",
            "Epoch: 9, Loss: 0.0664, Accuracy: 97.58%\n",
            "Epoch: 10, Loss: 0.0600, Accuracy: 97.84%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for x,y in train_loader:\n",
        "#   total_epoch_loss = 0\n",
        "#   x,y = x.to(device),y.to(device)\n",
        "\n",
        "#   #forward prop\n",
        "#   outputs = vgg16(x)\n",
        "#   #calculate loss\n",
        "#   loss = criterion(outputs,y)\n",
        "#   #back prop\n",
        "#   optimizer.zero_grad()\n",
        "#   loss.backward()\n",
        "\n",
        "#   #update grads\n",
        "#   optimizer.step()\n",
        "\n",
        "#   total_epoch_loss = total_epoch_loss + loss.item()\n",
        "# avg_loss = total_epoch_loss/len(train_loader)\n",
        "# print(f\"Epoch: {epochs+1},loss:{avg_loss}\")"
      ],
      "metadata": {
        "id": "eRZ2FaTSW_TW"
      },
      "execution_count": 23,
      "outputs": []
    }
  ]
}